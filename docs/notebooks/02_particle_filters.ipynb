{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Particle Filters: Sequential Monte Carlo Methods\n\nThis notebook covers particle filtering techniques for nonlinear, non-Gaussian state estimation. We explore:\n\n1. **Bootstrap Particle Filter** - The fundamental SIR algorithm\n2. **Resampling Strategies** - Multinomial, systematic, residual, stratified\n3. **Degeneracy and Effective Sample Size** - Diagnosing filter health\n4. **Effect of Particle Count** - Accuracy vs computation trade-offs\n\n## Prerequisites\n\n```bash\npip install nrl-tracker plotly numpy\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom pytcl.dynamic_estimation.particle_filters import (\n    bootstrap_pf_predict, bootstrap_pf_update,\n    resample_multinomial, resample_systematic, resample_residual, resample_stratified,\n    effective_sample_size, particle_mean, particle_covariance,\n    initialize_particles, gaussian_likelihood,\n)\n\nnp.random.seed(42)\n\n# Plotly dark theme template\ndark_template = go.layout.Template()\ndark_template.layout = go.Layout(\n    paper_bgcolor='#0d1117',\n    plot_bgcolor='#0d1117',\n    font=dict(color='#e6edf3'),\n    xaxis=dict(gridcolor='#30363d', zerolinecolor='#30363d'),\n    yaxis=dict(gridcolor='#30363d', zerolinecolor='#30363d'),\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bootstrap Particle Filter\n",
    "\n",
    "The bootstrap particle filter (Sequential Importance Resampling) represents the posterior distribution using a set of weighted samples:\n",
    "\n",
    "$$p(x_k | z_{1:k}) \\approx \\sum_{i=1}^{N} w_k^{(i)} \\delta(x - x_k^{(i)})$$\n",
    "\n",
    "### Algorithm:\n",
    "1. **Prediction**: Propagate particles through dynamics\n",
    "2. **Update**: Compute likelihood weights\n",
    "3. **Resample**: Eliminate low-weight particles\n",
    "\n",
    "### Example: Highly Nonlinear System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nonlinear state-space model\n",
    "def f_nonlinear(x, k):\n",
    "    \"\"\"Highly nonlinear dynamics.\"\"\"\n",
    "    return x / 2 + 25 * x / (1 + x**2) + 8 * np.cos(1.2 * k)\n",
    "\n",
    "def h_nonlinear(x):\n",
    "    \"\"\"Nonlinear measurement.\"\"\"\n",
    "    return x**2 / 20\n",
    "\n",
    "# Noise parameters\n",
    "Q = 10.0  # Process noise variance\n",
    "R = 1.0   # Measurement noise variance\n",
    "\n",
    "# Generate true trajectory and measurements\n",
    "n_steps = 100\n",
    "true_states = [0.0]\n",
    "measurements = []\n",
    "\n",
    "for k in range(n_steps):\n",
    "    # Propagate state\n",
    "    x_new = f_nonlinear(true_states[-1], k) + np.random.normal(0, np.sqrt(Q))\n",
    "    true_states.append(x_new)\n",
    "    \n",
    "    # Generate measurement\n",
    "    z = h_nonlinear(x_new) + np.random.normal(0, np.sqrt(R))\n",
    "    measurements.append(z)\n",
    "\n",
    "true_states = np.array(true_states)\n",
    "measurements = np.array(measurements)\n",
    "\n",
    "print(f\"State range: [{true_states.min():.1f}, {true_states.max():.1f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Particle filter parameters\n",
    "N_particles = 500\n",
    "\n",
    "# Initialize particles from prior\n",
    "particles = np.random.normal(0, np.sqrt(10), N_particles)  # Initial uncertainty\n",
    "weights = np.ones(N_particles) / N_particles\n",
    "\n",
    "# Storage for results\n",
    "pf_estimates = [np.average(particles, weights=weights)]\n",
    "pf_variances = [np.average((particles - pf_estimates[0])**2, weights=weights)]\n",
    "ess_history = [effective_sample_size(weights)]\n",
    "\n",
    "print(f\"Initialized {N_particles} particles\")\n",
    "print(f\"Initial ESS: {ess_history[0]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run particle filter\n",
    "ESS_threshold = N_particles / 2  # Resample when ESS drops below this\n",
    "\n",
    "for k, z in enumerate(measurements):\n",
    "    # Prediction: propagate particles\n",
    "    particles_pred = np.array([f_nonlinear(p, k) for p in particles])\n",
    "    particles_pred += np.random.normal(0, np.sqrt(Q), N_particles)\n",
    "    \n",
    "    # Update: compute likelihood weights\n",
    "    z_pred = np.array([h_nonlinear(p) for p in particles_pred])\n",
    "    likelihoods = np.exp(-0.5 * (z - z_pred)**2 / R)\n",
    "    weights = weights * likelihoods\n",
    "    weights = weights / np.sum(weights)  # Normalize\n",
    "    \n",
    "    # Compute ESS\n",
    "    ess = effective_sample_size(weights)\n",
    "    ess_history.append(ess)\n",
    "    \n",
    "    # Resample if needed\n",
    "    if ess < ESS_threshold:\n",
    "        indices = resample_systematic(weights)\n",
    "        particles = particles_pred[indices]\n",
    "        weights = np.ones(N_particles) / N_particles\n",
    "    else:\n",
    "        particles = particles_pred\n",
    "    \n",
    "    # Store estimates\n",
    "    mean = np.average(particles, weights=weights)\n",
    "    var = np.average((particles - mean)**2, weights=weights)\n",
    "    pf_estimates.append(mean)\n",
    "    pf_variances.append(var)\n",
    "\n",
    "pf_estimates = np.array(pf_estimates)\n",
    "pf_variances = np.array(pf_variances)\n",
    "\n",
    "print(f\"Minimum ESS: {min(ess_history):.1f}\")\n",
    "print(f\"Resampling events: {sum(1 for e in ess_history if e < ESS_threshold)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualization\nfig = make_subplots(\n    rows=3, cols=1,\n    subplot_titles=('Particle Filter State Estimation', \n                    f'Absolute Error (RMSE: {np.sqrt(np.mean((pf_estimates - true_states)**2)):.3f})',\n                    'Effective Sample Size'),\n    vertical_spacing=0.1\n)\n\ntime = np.arange(len(true_states))\n\n# State estimation\nfig.add_trace(\n    go.Scatter(x=time, y=pf_estimates + 2*np.sqrt(pf_variances), mode='lines',\n               line=dict(width=0), showlegend=False, hoverinfo='skip'),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Scatter(x=time, y=pf_estimates - 2*np.sqrt(pf_variances), mode='lines',\n               fill='tonexty', fillcolor='rgba(0, 212, 255, 0.3)',\n               line=dict(width=0), name='±2σ'),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Scatter(x=time, y=true_states, mode='lines',\n               name='True state', line=dict(color='#00ff88', width=2)),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Scatter(x=time, y=pf_estimates, mode='lines',\n               name='PF estimate', line=dict(color='#00d4ff', width=1.5, dash='dash')),\n    row=1, col=1\n)\n\n# Estimation error\nerror = np.abs(pf_estimates - true_states)\nfig.add_trace(\n    go.Scatter(x=time, y=error, mode='lines',\n               name='|Error|', line=dict(color='#ff4757', width=1.5)),\n    row=2, col=1\n)\n\n# Effective sample size\nfig.add_trace(\n    go.Scatter(x=time, y=ess_history, mode='lines',\n               name='ESS', line=dict(color='#a855f7', width=1.5)),\n    row=3, col=1\n)\nfig.add_trace(\n    go.Scatter(x=[time[0], time[-1]], y=[ESS_threshold, ESS_threshold], mode='lines',\n               name=f'Threshold ({ESS_threshold:.0f})', line=dict(color='#ff4757', dash='dash')),\n    row=3, col=1\n)\n\nfig.update_layout(\n    template=dark_template,\n    height=700,\n    showlegend=True,\n    legend=dict(x=1.02, y=0.5)\n)\nfig.update_yaxes(title_text='State', row=1, col=1)\nfig.update_yaxes(title_text='|Error|', row=2, col=1)\nfig.update_yaxes(title_text='ESS', row=3, col=1)\nfig.update_xaxes(title_text='Time step', row=3, col=1)\n\nfig.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Resampling Strategies Comparison\n",
    "\n",
    "Different resampling methods have different variance properties:\n",
    "\n",
    "| Method | Variance | Computation | Notes |\n",
    "|--------|----------|-------------|-------|\n",
    "| Multinomial | High | O(N log N) | Simple, but high variance |\n",
    "| Systematic | Low | O(N) | Single random number, good balance |\n",
    "| Stratified | Low | O(N) | N random numbers, slightly better |\n",
    "| Residual | Medium | O(N) | Deterministic + random hybrid |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare resampling methods\n",
    "def run_pf_with_resampler(resampler, name):\n",
    "    \"\"\"Run particle filter with specified resampling method.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    particles = np.random.normal(0, np.sqrt(10), N_particles)\n",
    "    weights = np.ones(N_particles) / N_particles\n",
    "    estimates = [np.average(particles, weights=weights)]\n",
    "    \n",
    "    for k, z in enumerate(measurements):\n",
    "        # Predict\n",
    "        particles_pred = np.array([f_nonlinear(p, k) for p in particles])\n",
    "        particles_pred += np.random.normal(0, np.sqrt(Q), N_particles)\n",
    "        \n",
    "        # Update weights\n",
    "        z_pred = np.array([h_nonlinear(p) for p in particles_pred])\n",
    "        likelihoods = np.exp(-0.5 * (z - z_pred)**2 / R)\n",
    "        weights = weights * likelihoods\n",
    "        weights = weights / np.sum(weights)\n",
    "        \n",
    "        # Resample\n",
    "        ess = effective_sample_size(weights)\n",
    "        if ess < ESS_threshold:\n",
    "            indices = resampler(weights)\n",
    "            particles = particles_pred[indices]\n",
    "            weights = np.ones(N_particles) / N_particles\n",
    "        else:\n",
    "            particles = particles_pred\n",
    "        \n",
    "        estimates.append(np.average(particles, weights=weights))\n",
    "    \n",
    "    rmse = np.sqrt(np.mean((np.array(estimates) - true_states)**2))\n",
    "    return np.array(estimates), rmse\n",
    "\n",
    "# Run with each resampler\n",
    "resamplers = [\n",
    "    (resample_multinomial, 'Multinomial'),\n",
    "    (resample_systematic, 'Systematic'),\n",
    "    (resample_stratified, 'Stratified'),\n",
    "    (resample_residual, 'Residual'),\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for resampler, name in resamplers:\n",
    "    est, rmse = run_pf_with_resampler(resampler, name)\n",
    "    results[name] = {'estimates': est, 'rmse': rmse}\n",
    "    print(f\"{name:12s}: RMSE = {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize differences (use subset for clarity)\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(x=time, y=true_states, mode='lines',\n               name='True', line=dict(color='white', width=2))\n)\n\ncolors = ['#00d4ff', '#ff4757', '#00ff88', '#ffb800']\nfor (name, data), color in zip(results.items(), colors):\n    fig.add_trace(\n        go.Scatter(x=time, y=data['estimates'], mode='lines',\n                   name=f\"{name} (RMSE={data['rmse']:.3f})\",\n                   line=dict(color=color, width=1.5, dash='dash'), opacity=0.8)\n    )\n\nfig.update_layout(\n    template=dark_template,\n    title='Resampling Method Comparison',\n    xaxis_title='Time step',\n    yaxis_title='State',\n    xaxis=dict(range=[0, 30]),  # Zoom in for detail\n    height=450\n)\nfig.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Particle Degeneracy and Weight Distribution\n",
    "\n",
    "A key challenge in particle filtering is **degeneracy**: after a few iterations, most particles have negligible weight. The Effective Sample Size (ESS) measures this:\n",
    "\n",
    "$$\\text{ESS} = \\frac{1}{\\sum_{i=1}^{N} (w^{(i)})^2}$$\n",
    "\n",
    "When ESS drops significantly below N, resampling is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate weight degeneracy without resampling\nnp.random.seed(123)\nparticles = np.random.normal(0, np.sqrt(10), N_particles)\nweights = np.ones(N_particles) / N_particles\n\ness_no_resample = [effective_sample_size(weights)]\nweight_entropy = []\n\nfor k, z in enumerate(measurements[:50]):  # First 50 steps\n    # Predict\n    particles = np.array([f_nonlinear(p, k) for p in particles])\n    particles += np.random.normal(0, np.sqrt(Q), N_particles)\n    \n    # Update WITHOUT resampling\n    z_pred = np.array([h_nonlinear(p) for p in particles])\n    likelihoods = np.exp(-0.5 * (z - z_pred)**2 / R)\n    weights = weights * likelihoods\n    weights = weights / np.sum(weights)\n    \n    ess_no_resample.append(effective_sample_size(weights))\n    \n    # Entropy of weights (measure of spread)\n    w_nonzero = weights[weights > 1e-300]\n    entropy = -np.sum(w_nonzero * np.log(w_nonzero))\n    weight_entropy.append(entropy)\n\nfig = make_subplots(rows=1, cols=2, \n                    subplot_titles=('ESS Collapse Without Resampling', 'Weight Distribution Entropy'))\n\nfig.add_trace(\n    go.Scatter(x=list(range(len(ess_no_resample))), y=ess_no_resample, mode='lines',\n               name='ESS', line=dict(color='#00d4ff', width=2)),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Scatter(x=[0, len(ess_no_resample)], y=[1, 1], mode='lines',\n               name='Complete degeneracy', line=dict(color='#ff4757', dash='dash')),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=list(range(len(weight_entropy))), y=weight_entropy, mode='lines',\n               name='Entropy', line=dict(color='#00ff88', width=2)),\n    row=1, col=2\n)\n\nfig.update_layout(\n    template=dark_template,\n    height=350,\n    showlegend=True\n)\nfig.update_yaxes(type='log', title_text='ESS (log scale)', row=1, col=1)\nfig.update_yaxes(title_text='Weight Entropy', row=1, col=2)\nfig.update_xaxes(title_text='Time step', row=1, col=1)\nfig.update_xaxes(title_text='Time step', row=1, col=2)\n\nfig.show()\n\nprint(f\"ESS at step 0: {ess_no_resample[0]:.1f}\")\nprint(f\"ESS at step 10: {ess_no_resample[10]:.2f}\")\nprint(f\"ESS at step 50: {ess_no_resample[50]:.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Effect of Number of Particles\n",
    "\n",
    "More particles generally lead to better estimation, but with diminishing returns. The computational cost scales linearly with particle count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "particle_counts = [50, 100, 200, 500, 1000, 2000]\n",
    "rmse_vs_particles = []\n",
    "time_vs_particles = []\n",
    "\n",
    "for N in particle_counts:\n",
    "    np.random.seed(42)\n",
    "    particles = np.random.normal(0, np.sqrt(10), N)\n",
    "    weights = np.ones(N) / N\n",
    "    estimates = []\n",
    "    \n",
    "    start = time.time()\n",
    "    for k, z in enumerate(measurements):\n",
    "        particles = np.array([f_nonlinear(p, k) for p in particles])\n",
    "        particles += np.random.normal(0, np.sqrt(Q), N)\n",
    "        \n",
    "        z_pred = np.array([h_nonlinear(p) for p in particles])\n",
    "        likelihoods = np.exp(-0.5 * (z - z_pred)**2 / R)\n",
    "        weights = weights * likelihoods\n",
    "        weights = weights / np.sum(weights)\n",
    "        \n",
    "        if effective_sample_size(weights) < N/2:\n",
    "            indices = resample_systematic(weights)\n",
    "            particles = particles[indices]\n",
    "            weights = np.ones(N) / N\n",
    "        \n",
    "        estimates.append(np.average(particles, weights=weights))\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    rmse = np.sqrt(np.mean((np.array(estimates) - true_states[1:])**2))\n",
    "    rmse_vs_particles.append(rmse)\n",
    "    time_vs_particles.append(elapsed)\n",
    "    print(f\"N={N:5d}: RMSE={rmse:.4f}, Time={elapsed:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig = make_subplots(rows=1, cols=2, \n                    subplot_titles=('Accuracy vs Particle Count', 'Computation Time vs Particle Count'))\n\nfig.add_trace(\n    go.Scatter(x=particle_counts, y=rmse_vs_particles, mode='lines+markers',\n               name='RMSE', line=dict(color='#00d4ff', width=2),\n               marker=dict(size=8)),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=particle_counts, y=time_vs_particles, mode='lines+markers',\n               name='Time', line=dict(color='#ff4757', width=2),\n               marker=dict(size=8)),\n    row=1, col=2\n)\n\nfig.update_layout(\n    template=dark_template,\n    height=350,\n    showlegend=False\n)\nfig.update_xaxes(type='log', title_text='Number of Particles', row=1, col=1)\nfig.update_xaxes(type='log', title_text='Number of Particles', row=1, col=2)\nfig.update_yaxes(title_text='RMSE', row=1, col=1)\nfig.update_yaxes(type='log', title_text='Computation Time (s)', row=1, col=2)\n\nfig.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. **Particle filters** handle arbitrary nonlinear and non-Gaussian systems\n",
    "2. **Resampling** is critical to prevent weight degeneracy\n",
    "3. **Systematic resampling** offers the best variance-computation trade-off\n",
    "4. **ESS monitoring** helps detect filter health issues\n",
    "5. **More particles** improve accuracy but increase computation\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Implement adaptive resampling that adjusts the threshold based on recent ESS history\n",
    "2. Add outlier measurements and observe how the filter handles them\n",
    "3. Implement a regularized particle filter to improve diversity\n",
    "4. Compare particle filter performance with EKF/UKF on this nonlinear problem\n",
    "\n",
    "## References\n",
    "\n",
    "1. Arulampalam, M. S., et al. (2002). A tutorial on particle filters. *IEEE TSP*.\n",
    "2. Doucet, A., & Johansen, A. M. (2009). A tutorial on particle filtering. *Handbook of Nonlinear Filtering*.\n",
    "3. Ristic, B., Arulampalam, S., & Gordon, N. (2004). *Beyond the Kalman Filter*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
