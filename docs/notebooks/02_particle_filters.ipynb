{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Filters: Sequential Monte Carlo Methods\n",
    "\n",
    "This notebook covers particle filtering techniques for nonlinear, non-Gaussian state estimation. We explore:\n",
    "\n",
    "1. **Bootstrap Particle Filter** - The fundamental SIR algorithm\n",
    "2. **Resampling Strategies** - Multinomial, systematic, residual, stratified\n",
    "3. **Degeneracy and Effective Sample Size** - Diagnosing filter health\n",
    "4. **Rao-Blackwellized Particle Filter** - Hybrid particle-Kalman approach\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "pip install nrl-tracker matplotlib numpy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pytcl.dynamic_estimation.particle_filters import (\n",
    "    bootstrap_pf_predict, bootstrap_pf_update,\n",
    "    resample_multinomial, resample_systematic, resample_residual, resample_stratified,\n",
    "    effective_sample_size, particle_mean, particle_covariance,\n",
    "    initialize_particles, gaussian_likelihood,\n",
    ")\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bootstrap Particle Filter\n",
    "\n",
    "The bootstrap particle filter (Sequential Importance Resampling) represents the posterior distribution using a set of weighted samples:\n",
    "\n",
    "$$p(x_k | z_{1:k}) \\approx \\sum_{i=1}^{N} w_k^{(i)} \\delta(x - x_k^{(i)})$$\n",
    "\n",
    "### Algorithm:\n",
    "1. **Prediction**: Propagate particles through dynamics\n",
    "2. **Update**: Compute likelihood weights\n",
    "3. **Resample**: Eliminate low-weight particles\n",
    "\n",
    "### Example: Highly Nonlinear System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nonlinear state-space model\n",
    "def f_nonlinear(x, k):\n",
    "    \"\"\"Highly nonlinear dynamics.\"\"\"\n",
    "    return x / 2 + 25 * x / (1 + x**2) + 8 * np.cos(1.2 * k)\n",
    "\n",
    "def h_nonlinear(x):\n",
    "    \"\"\"Nonlinear measurement.\"\"\"\n",
    "    return x**2 / 20\n",
    "\n",
    "# Noise parameters\n",
    "Q = 10.0  # Process noise variance\n",
    "R = 1.0   # Measurement noise variance\n",
    "\n",
    "# Generate true trajectory and measurements\n",
    "n_steps = 100\n",
    "true_states = [0.0]\n",
    "measurements = []\n",
    "\n",
    "for k in range(n_steps):\n",
    "    # Propagate state\n",
    "    x_new = f_nonlinear(true_states[-1], k) + np.random.normal(0, np.sqrt(Q))\n",
    "    true_states.append(x_new)\n",
    "    \n",
    "    # Generate measurement\n",
    "    z = h_nonlinear(x_new) + np.random.normal(0, np.sqrt(R))\n",
    "    measurements.append(z)\n",
    "\n",
    "true_states = np.array(true_states)\n",
    "measurements = np.array(measurements)\n",
    "\n",
    "print(f\"State range: [{true_states.min():.1f}, {true_states.max():.1f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Particle filter parameters\n",
    "N_particles = 500\n",
    "\n",
    "# Initialize particles from prior\n",
    "particles = np.random.normal(0, np.sqrt(10), N_particles)  # Initial uncertainty\n",
    "weights = np.ones(N_particles) / N_particles\n",
    "\n",
    "# Storage for results\n",
    "pf_estimates = [np.average(particles, weights=weights)]\n",
    "pf_variances = [np.average((particles - pf_estimates[0])**2, weights=weights)]\n",
    "ess_history = [effective_sample_size(weights)]\n",
    "\n",
    "print(f\"Initialized {N_particles} particles\")\n",
    "print(f\"Initial ESS: {ess_history[0]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run particle filter\n",
    "ESS_threshold = N_particles / 2  # Resample when ESS drops below this\n",
    "\n",
    "for k, z in enumerate(measurements):\n",
    "    # Prediction: propagate particles\n",
    "    particles_pred = np.array([f_nonlinear(p, k) for p in particles])\n",
    "    particles_pred += np.random.normal(0, np.sqrt(Q), N_particles)\n",
    "    \n",
    "    # Update: compute likelihood weights\n",
    "    z_pred = np.array([h_nonlinear(p) for p in particles_pred])\n",
    "    likelihoods = np.exp(-0.5 * (z - z_pred)**2 / R)\n",
    "    weights = weights * likelihoods\n",
    "    weights = weights / np.sum(weights)  # Normalize\n",
    "    \n",
    "    # Compute ESS\n",
    "    ess = effective_sample_size(weights)\n",
    "    ess_history.append(ess)\n",
    "    \n",
    "    # Resample if needed\n",
    "    if ess < ESS_threshold:\n",
    "        indices = resample_systematic(weights)\n",
    "        particles = particles_pred[indices]\n",
    "        weights = np.ones(N_particles) / N_particles\n",
    "    else:\n",
    "        particles = particles_pred\n",
    "    \n",
    "    # Store estimates\n",
    "    mean = np.average(particles, weights=weights)\n",
    "    var = np.average((particles - mean)**2, weights=weights)\n",
    "    pf_estimates.append(mean)\n",
    "    pf_variances.append(var)\n",
    "\n",
    "pf_estimates = np.array(pf_estimates)\n",
    "pf_variances = np.array(pf_variances)\n",
    "\n",
    "print(f\"Minimum ESS: {min(ess_history):.1f}\")\n",
    "print(f\"Resampling events: {sum(1 for e in ess_history if e < ESS_threshold)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n",
    "\n",
    "# State estimation\n",
    "ax = axes[0]\n",
    "time = np.arange(len(true_states))\n",
    "ax.plot(time, true_states, 'g-', linewidth=2, label='True state')\n",
    "ax.plot(time, pf_estimates, 'b--', linewidth=1.5, label='PF estimate')\n",
    "ax.fill_between(time, \n",
    "                pf_estimates - 2*np.sqrt(pf_variances),\n",
    "                pf_estimates + 2*np.sqrt(pf_variances),\n",
    "                alpha=0.3, label='±2σ')\n",
    "ax.set_ylabel('State')\n",
    "ax.set_title('Particle Filter State Estimation')\n",
    "ax.legend()\n",
    "\n",
    "# Estimation error\n",
    "ax = axes[1]\n",
    "error = np.abs(pf_estimates - true_states)\n",
    "ax.plot(time, error, 'r-')\n",
    "ax.set_ylabel('|Error|')\n",
    "ax.set_title(f'Absolute Error (RMSE: {np.sqrt(np.mean(error**2)):.3f})')\n",
    "\n",
    "# Effective sample size\n",
    "ax = axes[2]\n",
    "ax.plot(time, ess_history, 'purple')\n",
    "ax.axhline(ESS_threshold, color='r', linestyle='--', label=f'Threshold ({ESS_threshold:.0f})')\n",
    "ax.set_xlabel('Time step')\n",
    "ax.set_ylabel('ESS')\n",
    "ax.set_title('Effective Sample Size')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Resampling Strategies Comparison\n",
    "\n",
    "Different resampling methods have different variance properties:\n",
    "\n",
    "| Method | Variance | Computation | Notes |\n",
    "|--------|----------|-------------|-------|\n",
    "| Multinomial | High | O(N log N) | Simple, but high variance |\n",
    "| Systematic | Low | O(N) | Single random number, good balance |\n",
    "| Stratified | Low | O(N) | N random numbers, slightly better |\n",
    "| Residual | Medium | O(N) | Deterministic + random hybrid |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare resampling methods\n",
    "def run_pf_with_resampler(resampler, name):\n",
    "    \"\"\"Run particle filter with specified resampling method.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    particles = np.random.normal(0, np.sqrt(10), N_particles)\n",
    "    weights = np.ones(N_particles) / N_particles\n",
    "    estimates = [np.average(particles, weights=weights)]\n",
    "    \n",
    "    for k, z in enumerate(measurements):\n",
    "        # Predict\n",
    "        particles_pred = np.array([f_nonlinear(p, k) for p in particles])\n",
    "        particles_pred += np.random.normal(0, np.sqrt(Q), N_particles)\n",
    "        \n",
    "        # Update weights\n",
    "        z_pred = np.array([h_nonlinear(p) for p in particles_pred])\n",
    "        likelihoods = np.exp(-0.5 * (z - z_pred)**2 / R)\n",
    "        weights = weights * likelihoods\n",
    "        weights = weights / np.sum(weights)\n",
    "        \n",
    "        # Resample\n",
    "        ess = effective_sample_size(weights)\n",
    "        if ess < ESS_threshold:\n",
    "            indices = resampler(weights)\n",
    "            particles = particles_pred[indices]\n",
    "            weights = np.ones(N_particles) / N_particles\n",
    "        else:\n",
    "            particles = particles_pred\n",
    "        \n",
    "        estimates.append(np.average(particles, weights=weights))\n",
    "    \n",
    "    rmse = np.sqrt(np.mean((np.array(estimates) - true_states)**2))\n",
    "    return np.array(estimates), rmse\n",
    "\n",
    "# Run with each resampler\n",
    "resamplers = [\n",
    "    (resample_multinomial, 'Multinomial'),\n",
    "    (resample_systematic, 'Systematic'),\n",
    "    (resample_stratified, 'Stratified'),\n",
    "    (resample_residual, 'Residual'),\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for resampler, name in resamplers:\n",
    "    est, rmse = run_pf_with_resampler(resampler, name)\n",
    "    results[name] = {'estimates': est, 'rmse': rmse}\n",
    "    print(f\"{name:12s}: RMSE = {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize differences (use subset for clarity)\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.plot(time, true_states, 'k-', linewidth=2, label='True')\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "for (name, data), color in zip(results.items(), colors):\n",
    "    ax.plot(time, data['estimates'], '--', color=color, alpha=0.7,\n",
    "            label=f\"{name} (RMSE={data['rmse']:.3f})\")\n",
    "\n",
    "ax.set_xlabel('Time step')\n",
    "ax.set_ylabel('State')\n",
    "ax.set_title('Resampling Method Comparison')\n",
    "ax.legend()\n",
    "ax.set_xlim([0, 30])  # Zoom in for detail\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Particle Degeneracy and Weight Distribution\n",
    "\n",
    "A key challenge in particle filtering is **degeneracy**: after a few iterations, most particles have negligible weight. The Effective Sample Size (ESS) measures this:\n",
    "\n",
    "$$\\text{ESS} = \\frac{1}{\\sum_{i=1}^{N} (w^{(i)})^2}$$\n",
    "\n",
    "When ESS drops significantly below N, resampling is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate weight degeneracy without resampling\n",
    "np.random.seed(123)\n",
    "particles = np.random.normal(0, np.sqrt(10), N_particles)\n",
    "weights = np.ones(N_particles) / N_particles\n",
    "\n",
    "ess_no_resample = [effective_sample_size(weights)]\n",
    "weight_entropy = []\n",
    "\n",
    "for k, z in enumerate(measurements[:50]):  # First 50 steps\n",
    "    # Predict\n",
    "    particles = np.array([f_nonlinear(p, k) for p in particles])\n",
    "    particles += np.random.normal(0, np.sqrt(Q), N_particles)\n",
    "    \n",
    "    # Update WITHOUT resampling\n",
    "    z_pred = np.array([h_nonlinear(p) for p in particles])\n",
    "    likelihoods = np.exp(-0.5 * (z - z_pred)**2 / R)\n",
    "    weights = weights * likelihoods\n",
    "    weights = weights / np.sum(weights)\n",
    "    \n",
    "    ess_no_resample.append(effective_sample_size(weights))\n",
    "    \n",
    "    # Entropy of weights (measure of spread)\n",
    "    w_nonzero = weights[weights > 1e-300]\n",
    "    entropy = -np.sum(w_nonzero * np.log(w_nonzero))\n",
    "    weight_entropy.append(entropy)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.semilogy(ess_no_resample, 'b-')\n",
    "ax.axhline(1, color='r', linestyle='--', label='Complete degeneracy')\n",
    "ax.set_xlabel('Time step')\n",
    "ax.set_ylabel('ESS (log scale)')\n",
    "ax.set_title('ESS Collapse Without Resampling')\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(weight_entropy, 'g-')\n",
    "ax.set_xlabel('Time step')\n",
    "ax.set_ylabel('Weight Entropy')\n",
    "ax.set_title('Weight Distribution Entropy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ESS at step 0: {ess_no_resample[0]:.1f}\")\n",
    "print(f\"ESS at step 10: {ess_no_resample[10]:.2f}\")\n",
    "print(f\"ESS at step 50: {ess_no_resample[50]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Effect of Number of Particles\n",
    "\n",
    "More particles generally lead to better estimation, but with diminishing returns. The computational cost scales linearly with particle count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "particle_counts = [50, 100, 200, 500, 1000, 2000]\n",
    "rmse_vs_particles = []\n",
    "time_vs_particles = []\n",
    "\n",
    "for N in particle_counts:\n",
    "    np.random.seed(42)\n",
    "    particles = np.random.normal(0, np.sqrt(10), N)\n",
    "    weights = np.ones(N) / N\n",
    "    estimates = []\n",
    "    \n",
    "    start = time.time()\n",
    "    for k, z in enumerate(measurements):\n",
    "        particles = np.array([f_nonlinear(p, k) for p in particles])\n",
    "        particles += np.random.normal(0, np.sqrt(Q), N)\n",
    "        \n",
    "        z_pred = np.array([h_nonlinear(p) for p in particles])\n",
    "        likelihoods = np.exp(-0.5 * (z - z_pred)**2 / R)\n",
    "        weights = weights * likelihoods\n",
    "        weights = weights / np.sum(weights)\n",
    "        \n",
    "        if effective_sample_size(weights) < N/2:\n",
    "            indices = resample_systematic(weights)\n",
    "            particles = particles[indices]\n",
    "            weights = np.ones(N) / N\n",
    "        \n",
    "        estimates.append(np.average(particles, weights=weights))\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    rmse = np.sqrt(np.mean((np.array(estimates) - true_states[1:])**2))\n",
    "    rmse_vs_particles.append(rmse)\n",
    "    time_vs_particles.append(elapsed)\n",
    "    print(f\"N={N:5d}: RMSE={rmse:.4f}, Time={elapsed:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.semilogx(particle_counts, rmse_vs_particles, 'bo-', markersize=8)\n",
    "ax.set_xlabel('Number of Particles')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('Accuracy vs Particle Count')\n",
    "ax.grid(True)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.loglog(particle_counts, time_vs_particles, 'ro-', markersize=8)\n",
    "ax.set_xlabel('Number of Particles')\n",
    "ax.set_ylabel('Computation Time (s)')\n",
    "ax.set_title('Computation Time vs Particle Count')\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. **Particle filters** handle arbitrary nonlinear and non-Gaussian systems\n",
    "2. **Resampling** is critical to prevent weight degeneracy\n",
    "3. **Systematic resampling** offers the best variance-computation trade-off\n",
    "4. **ESS monitoring** helps detect filter health issues\n",
    "5. **More particles** improve accuracy but increase computation\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Implement adaptive resampling that adjusts the threshold based on recent ESS history\n",
    "2. Add outlier measurements and observe how the filter handles them\n",
    "3. Implement a regularized particle filter to improve diversity\n",
    "4. Compare particle filter performance with EKF/UKF on this nonlinear problem\n",
    "\n",
    "## References\n",
    "\n",
    "1. Arulampalam, M. S., et al. (2002). A tutorial on particle filters. *IEEE TSP*.\n",
    "2. Doucet, A., & Johansen, A. M. (2009). A tutorial on particle filtering. *Handbook of Nonlinear Filtering*.\n",
    "3. Ristic, B., Arulampalam, S., & Gordon, N. (2004). *Beyond the Kalman Filter*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
