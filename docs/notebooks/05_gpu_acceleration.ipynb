{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# GPU Acceleration: CuPy Integration for Tracking\n",
    "\n",
    "This notebook demonstrates GPU acceleration techniques for tracking algorithms using CuPy. We cover:\n",
    "\n",
    "1. **CuPy Basics** - GPU array operations and NumPy compatibility\n",
    "2. **Accelerating Matrix Operations** - Core operations for Kalman filters\n",
    "3. **Batch Processing** - Processing multiple tracks in parallel\n",
    "4. **Particle Filter Acceleration** - GPU-based resampling and weight computation\n",
    "5. **Performance Comparison** - Benchmarking CPU vs GPU\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "# GPU-enabled version\n",
    "pip install cupy-cuda12x  # For CUDA 12.x\n",
    "# or pip install cupy-cuda11x for CUDA 11.x\n",
    "\n",
    "pip install nrl-tracker matplotlib numpy\n",
    "```\n",
    "\n",
    "**Note:** This notebook requires an NVIDIA GPU with CUDA support. If running without a GPU, code examples will demonstrate concepts with NumPy fallbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Check for GPU availability\n",
    "try:\n",
    "    import cupy as cp\n",
    "    GPU_AVAILABLE = True\n",
    "    print(f\"CuPy version: {cp.__version__}\")\n",
    "    print(f\"CUDA version: {cp.cuda.runtime.runtimeGetVersion()}\")\n",
    "    print(f\"GPU: {cp.cuda.Device().name}\")\n",
    "    print(f\"GPU memory: {cp.cuda.Device().mem_info[1] / 1e9:.1f} GB\")\n",
    "except ImportError:\n",
    "    GPU_AVAILABLE = False\n",
    "    print(\"CuPy not available. Running in CPU-only mode.\")\n",
    "    print(\"Install with: pip install cupy-cuda12x\")\n",
    "\n",
    "# Use appropriate array module\n",
    "xp = cp if GPU_AVAILABLE else np\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. CuPy Basics\n",
    "\n",
    "CuPy provides a NumPy-compatible interface for GPU computing. Arrays and operations mirror NumPy exactly, making migration straightforward.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| NumPy | CuPy | Location |\n",
    "|-------|------|----------|\n",
    "| `numpy.array()` | `cupy.array()` | GPU |\n",
    "| `numpy.zeros()` | `cupy.zeros()` | GPU |\n",
    "| `numpy.linalg.inv()` | `cupy.linalg.inv()` | GPU |\n",
    "| Transfer to GPU | `cupy.asarray(np_array)` | CPU→GPU |\n",
    "| Transfer to CPU | `cupy.asnumpy(cp_array)` | GPU→CPU |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic array operations\n",
    "if GPU_AVAILABLE:\n",
    "    # Create arrays on GPU\n",
    "    a_gpu = cp.array([1, 2, 3, 4, 5])\n",
    "    b_gpu = cp.array([5, 4, 3, 2, 1])\n",
    "    \n",
    "    # Operations happen on GPU\n",
    "    c_gpu = a_gpu + b_gpu\n",
    "    d_gpu = cp.dot(a_gpu, b_gpu)\n",
    "    \n",
    "    print(f\"a + b = {c_gpu}\")\n",
    "    print(f\"a · b = {d_gpu}\")\n",
    "    print(f\"Array type: {type(c_gpu)}\")\n",
    "    \n",
    "    # Transfer back to CPU when needed\n",
    "    c_cpu = cp.asnumpy(c_gpu)\n",
    "    print(f\"Transferred type: {type(c_cpu)}\")\n",
    "else:\n",
    "    # NumPy fallback\n",
    "    a_cpu = np.array([1, 2, 3, 4, 5])\n",
    "    b_cpu = np.array([5, 4, 3, 2, 1])\n",
    "    print(f\"a + b = {a_cpu + b_cpu}\")\n",
    "    print(f\"a · b = {np.dot(a_cpu, b_cpu)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory management\n",
    "if GPU_AVAILABLE:\n",
    "    # Check memory before\n",
    "    mempool = cp.get_default_memory_pool()\n",
    "    print(f\"Memory used before: {mempool.used_bytes() / 1e6:.2f} MB\")\n",
    "    \n",
    "    # Create large array\n",
    "    large_array = cp.random.randn(10000, 10000)\n",
    "    print(f\"Memory after 10000x10000 float64: {mempool.used_bytes() / 1e6:.2f} MB\")\n",
    "    \n",
    "    # Free memory explicitly\n",
    "    del large_array\n",
    "    mempool.free_all_blocks()\n",
    "    print(f\"Memory after free: {mempool.used_bytes() / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Accelerating Matrix Operations\n",
    "\n",
    "Kalman filters involve repeated matrix operations that benefit from GPU parallelization:\n",
    "\n",
    "- Matrix multiplication: $P \\cdot H^T$\n",
    "- Matrix inversion: $(HPH^T + R)^{-1}$\n",
    "- Cholesky decomposition: $P = LL^T$\n",
    "\n",
    "Let's implement core Kalman filter operations for both CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman_predict(x, P, F, Q, xp=np):\n",
    "    \"\"\"\n",
    "    Kalman filter prediction step.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like\n",
    "        State estimate [n, 1].\n",
    "    P : array_like\n",
    "        State covariance [n, n].\n",
    "    F : array_like\n",
    "        State transition matrix [n, n].\n",
    "    Q : array_like\n",
    "        Process noise covariance [n, n].\n",
    "    xp : module\n",
    "        Array module (numpy or cupy).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x_pred : array\n",
    "        Predicted state.\n",
    "    P_pred : array\n",
    "        Predicted covariance.\n",
    "    \"\"\"\n",
    "    x_pred = xp.dot(F, x)\n",
    "    P_pred = xp.dot(xp.dot(F, P), F.T) + Q\n",
    "    return x_pred, P_pred\n",
    "\n",
    "\n",
    "def kalman_update(x, P, z, H, R, xp=np):\n",
    "    \"\"\"\n",
    "    Kalman filter update step.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like\n",
    "        Predicted state [n, 1].\n",
    "    P : array_like\n",
    "        Predicted covariance [n, n].\n",
    "    z : array_like\n",
    "        Measurement [m, 1].\n",
    "    H : array_like\n",
    "        Measurement matrix [m, n].\n",
    "    R : array_like\n",
    "        Measurement noise covariance [m, m].\n",
    "    xp : module\n",
    "        Array module (numpy or cupy).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x_upd : array\n",
    "        Updated state.\n",
    "    P_upd : array\n",
    "        Updated covariance.\n",
    "    \"\"\"\n",
    "    # Innovation\n",
    "    y = z - xp.dot(H, x)\n",
    "    \n",
    "    # Innovation covariance\n",
    "    S = xp.dot(xp.dot(H, P), H.T) + R\n",
    "    \n",
    "    # Kalman gain\n",
    "    K = xp.dot(xp.dot(P, H.T), xp.linalg.inv(S))\n",
    "    \n",
    "    # State update\n",
    "    x_upd = x + xp.dot(K, y)\n",
    "    \n",
    "    # Covariance update (Joseph form for numerical stability)\n",
    "    I = xp.eye(P.shape[0])\n",
    "    IKH = I - xp.dot(K, H)\n",
    "    P_upd = xp.dot(xp.dot(IKH, P), IKH.T) + xp.dot(xp.dot(K, R), K.T)\n",
    "    \n",
    "    return x_upd, P_upd\n",
    "\n",
    "\n",
    "print(\"Kalman filter functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Kalman filter on CPU and GPU\n",
    "state_dim = 4  # [x, vx, y, vy]\n",
    "meas_dim = 2   # [x, y]\n",
    "\n",
    "dt = 1.0\n",
    "\n",
    "# State transition (constant velocity)\n",
    "F = np.array([\n",
    "    [1, dt, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, dt],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "# Measurement matrix (observe position only)\n",
    "H = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 0, 1, 0]\n",
    "])\n",
    "\n",
    "# Process noise\n",
    "q = 0.1\n",
    "Q = q * np.array([\n",
    "    [dt**3/3, dt**2/2, 0, 0],\n",
    "    [dt**2/2, dt, 0, 0],\n",
    "    [0, 0, dt**3/3, dt**2/2],\n",
    "    [0, 0, dt**2/2, dt]\n",
    "])\n",
    "\n",
    "# Measurement noise\n",
    "R = 10 * np.eye(meas_dim)\n",
    "\n",
    "# Initial state and covariance\n",
    "x = np.array([0, 1, 0, 1]).reshape(-1, 1)\n",
    "P = 100 * np.eye(state_dim)\n",
    "\n",
    "# Measurement\n",
    "z = np.array([5, 5]).reshape(-1, 1)\n",
    "\n",
    "print(\"System matrices defined.\")\n",
    "print(f\"State dimension: {state_dim}\")\n",
    "print(f\"Measurement dimension: {meas_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on CPU\n",
    "x_pred_cpu, P_pred_cpu = kalman_predict(x, P, F, Q, xp=np)\n",
    "x_upd_cpu, P_upd_cpu = kalman_update(x_pred_cpu, P_pred_cpu, z, H, R, xp=np)\n",
    "\n",
    "print(\"CPU Results:\")\n",
    "print(f\"Predicted state: {x_pred_cpu.flatten()}\")\n",
    "print(f\"Updated state: {x_upd_cpu.flatten()}\")\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    # Transfer to GPU\n",
    "    x_gpu = cp.asarray(x)\n",
    "    P_gpu = cp.asarray(P)\n",
    "    F_gpu = cp.asarray(F)\n",
    "    Q_gpu = cp.asarray(Q)\n",
    "    H_gpu = cp.asarray(H)\n",
    "    R_gpu = cp.asarray(R)\n",
    "    z_gpu = cp.asarray(z)\n",
    "    \n",
    "    # Run on GPU\n",
    "    x_pred_gpu, P_pred_gpu = kalman_predict(x_gpu, P_gpu, F_gpu, Q_gpu, xp=cp)\n",
    "    x_upd_gpu, P_upd_gpu = kalman_update(x_pred_gpu, P_pred_gpu, z_gpu, H_gpu, R_gpu, xp=cp)\n",
    "    \n",
    "    print(\"\\nGPU Results:\")\n",
    "    print(f\"Predicted state: {cp.asnumpy(x_pred_gpu).flatten()}\")\n",
    "    print(f\"Updated state: {cp.asnumpy(x_upd_gpu).flatten()}\")\n",
    "    \n",
    "    # Verify results match\n",
    "    error = np.max(np.abs(cp.asnumpy(x_upd_gpu) - x_upd_cpu))\n",
    "    print(f\"\\nMax difference CPU vs GPU: {error:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 3. Batch Processing Multiple Tracks\n",
    "\n",
    "The real power of GPU acceleration comes from processing many tracks simultaneously. Instead of looping over tracks, we can stack them into batched arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_kalman_predict(x_batch, P_batch, F, Q, xp=np):\n",
    "    \"\"\"\n",
    "    Batch Kalman filter prediction.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_batch : array_like\n",
    "        Batch of states [batch_size, n, 1].\n",
    "    P_batch : array_like\n",
    "        Batch of covariances [batch_size, n, n].\n",
    "    F : array_like\n",
    "        State transition matrix [n, n] (shared).\n",
    "    Q : array_like\n",
    "        Process noise [n, n] (shared).\n",
    "    xp : module\n",
    "        Array module.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x_pred : array\n",
    "        Predicted states [batch_size, n, 1].\n",
    "    P_pred : array\n",
    "        Predicted covariances [batch_size, n, n].\n",
    "    \"\"\"\n",
    "    batch_size = x_batch.shape[0]\n",
    "    \n",
    "    # Vectorized prediction\n",
    "    # x_pred = F @ x for each batch element\n",
    "    x_pred = xp.einsum('ij,bjk->bik', F, x_batch)\n",
    "    \n",
    "    # P_pred = F @ P @ F.T + Q for each batch element\n",
    "    FP = xp.einsum('ij,bjk->bik', F, P_batch)\n",
    "    FPFt = xp.einsum('bij,kj->bik', FP, F)\n",
    "    P_pred = FPFt + Q\n",
    "    \n",
    "    return x_pred, P_pred\n",
    "\n",
    "\n",
    "def batch_kalman_update(x_batch, P_batch, z_batch, H, R, xp=np):\n",
    "    \"\"\"\n",
    "    Batch Kalman filter update.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_batch : array_like\n",
    "        Batch of predicted states [batch_size, n, 1].\n",
    "    P_batch : array_like\n",
    "        Batch of predicted covariances [batch_size, n, n].\n",
    "    z_batch : array_like\n",
    "        Batch of measurements [batch_size, m, 1].\n",
    "    H : array_like\n",
    "        Measurement matrix [m, n] (shared).\n",
    "    R : array_like\n",
    "        Measurement noise [m, m] (shared).\n",
    "    xp : module\n",
    "        Array module.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x_upd : array\n",
    "        Updated states [batch_size, n, 1].\n",
    "    P_upd : array\n",
    "        Updated covariances [batch_size, n, n].\n",
    "    \"\"\"\n",
    "    batch_size = x_batch.shape[0]\n",
    "    n = x_batch.shape[1]\n",
    "    \n",
    "    # Innovation: y = z - H @ x\n",
    "    Hx = xp.einsum('ij,bjk->bik', H, x_batch)\n",
    "    y = z_batch - Hx\n",
    "    \n",
    "    # Innovation covariance: S = H @ P @ H.T + R\n",
    "    HP = xp.einsum('ij,bjk->bik', H, P_batch)\n",
    "    HPHt = xp.einsum('bij,kj->bik', HP, H)\n",
    "    S = HPHt + R\n",
    "    \n",
    "    # Kalman gain: K = P @ H.T @ S^{-1}\n",
    "    PHt = xp.einsum('bij,kj->bik', P_batch, H)\n",
    "    S_inv = xp.linalg.inv(S)\n",
    "    K = xp.einsum('bij,bjk->bik', PHt, S_inv)\n",
    "    \n",
    "    # State update: x_upd = x + K @ y\n",
    "    Ky = xp.einsum('bij,bjk->bik', K, y)\n",
    "    x_upd = x_batch + Ky\n",
    "    \n",
    "    # Covariance update (simplified form)\n",
    "    I = xp.eye(n)\n",
    "    KH = xp.einsum('bij,jk->bik', K, H)\n",
    "    IKH = I - KH\n",
    "    P_upd = xp.einsum('bij,bjk->bik', IKH, P_batch)\n",
    "    \n",
    "    return x_upd, P_upd\n",
    "\n",
    "\n",
    "print(\"Batch Kalman functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark batch processing\n",
    "batch_sizes = [10, 100, 1000, 5000]\n",
    "\n",
    "if not GPU_AVAILABLE:\n",
    "    batch_sizes = [10, 100, 1000]  # Smaller sizes for CPU-only\n",
    "\n",
    "cpu_times = []\n",
    "gpu_times = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    # Create batch data\n",
    "    x_batch_np = np.random.randn(batch_size, state_dim, 1)\n",
    "    P_batch_np = np.tile(P.reshape(1, state_dim, state_dim), (batch_size, 1, 1))\n",
    "    z_batch_np = np.random.randn(batch_size, meas_dim, 1)\n",
    "    \n",
    "    # CPU timing\n",
    "    n_iterations = 100\n",
    "    start = time.time()\n",
    "    for _ in range(n_iterations):\n",
    "        x_pred, P_pred = batch_kalman_predict(x_batch_np, P_batch_np, F, Q, xp=np)\n",
    "        x_upd, P_upd = batch_kalman_update(x_pred, P_pred, z_batch_np, H, R, xp=np)\n",
    "    cpu_time = (time.time() - start) / n_iterations\n",
    "    cpu_times.append(cpu_time)\n",
    "    \n",
    "    if GPU_AVAILABLE:\n",
    "        # Transfer to GPU\n",
    "        x_batch_gpu = cp.asarray(x_batch_np)\n",
    "        P_batch_gpu = cp.asarray(P_batch_np)\n",
    "        z_batch_gpu = cp.asarray(z_batch_np)\n",
    "        \n",
    "        # Warm up\n",
    "        x_pred, P_pred = batch_kalman_predict(x_batch_gpu, P_batch_gpu, F_gpu, Q_gpu, xp=cp)\n",
    "        x_upd, P_upd = batch_kalman_update(x_pred, P_pred, z_batch_gpu, H_gpu, R_gpu, xp=cp)\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        \n",
    "        # GPU timing\n",
    "        start = time.time()\n",
    "        for _ in range(n_iterations):\n",
    "            x_pred, P_pred = batch_kalman_predict(x_batch_gpu, P_batch_gpu, F_gpu, Q_gpu, xp=cp)\n",
    "            x_upd, P_upd = batch_kalman_update(x_pred, P_pred, z_batch_gpu, H_gpu, R_gpu, xp=cp)\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        gpu_time = (time.time() - start) / n_iterations\n",
    "        gpu_times.append(gpu_time)\n",
    "        \n",
    "        speedup = cpu_time / gpu_time\n",
    "        print(f\"Batch size {batch_size:5d}: CPU={cpu_time*1e3:7.3f}ms, \"\n",
    "              f\"GPU={gpu_time*1e3:7.3f}ms, Speedup={speedup:.1f}x\")\n",
    "    else:\n",
    "        print(f\"Batch size {batch_size:5d}: CPU={cpu_time*1e3:7.3f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance\n",
    "if GPU_AVAILABLE and len(gpu_times) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    ax = axes[0]\n",
    "    ax.loglog(batch_sizes, np.array(cpu_times)*1e3, 'b-o', label='CPU (NumPy)', linewidth=2)\n",
    "    ax.loglog(batch_sizes, np.array(gpu_times)*1e3, 'r-s', label='GPU (CuPy)', linewidth=2)\n",
    "    ax.set_xlabel('Batch Size (number of tracks)')\n",
    "    ax.set_ylabel('Time per iteration (ms)')\n",
    "    ax.set_title('Kalman Filter Batch Performance')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    ax = axes[1]\n",
    "    speedups = np.array(cpu_times) / np.array(gpu_times)\n",
    "    ax.semilogx(batch_sizes, speedups, 'g-^', linewidth=2, markersize=10)\n",
    "    ax.axhline(y=1, color='k', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Batch Size (number of tracks)')\n",
    "    ax.set_ylabel('Speedup (CPU time / GPU time)')\n",
    "    ax.set_title('GPU Speedup Factor')\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.loglog(batch_sizes, np.array(cpu_times)*1e3, 'b-o', label='CPU (NumPy)', linewidth=2)\n",
    "    ax.set_xlabel('Batch Size')\n",
    "    ax.set_ylabel('Time (ms)')\n",
    "    ax.set_title('Kalman Filter CPU Performance')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 4. GPU-Accelerated Particle Filter\n",
    "\n",
    "Particle filters involve operations on large numbers of particles that parallelize well:\n",
    "\n",
    "1. **Particle propagation**: Apply dynamics to each particle\n",
    "2. **Weight computation**: Evaluate likelihood for each particle\n",
    "3. **Resampling**: Select particles based on weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def particle_filter_step(particles, weights, z, f_dynamics, h_meas, Q_std, R_std, xp=np):\n",
    "    \"\"\"\n",
    "    GPU-accelerated particle filter step.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    particles : array_like\n",
    "        Particle states [n_particles, state_dim].\n",
    "    weights : array_like\n",
    "        Particle weights [n_particles].\n",
    "    z : array_like\n",
    "        Measurement [meas_dim].\n",
    "    f_dynamics : callable\n",
    "        Dynamics function f(x) -> x_next.\n",
    "    h_meas : callable\n",
    "        Measurement function h(x) -> z_pred.\n",
    "    Q_std : float\n",
    "        Process noise standard deviation.\n",
    "    R_std : float\n",
    "        Measurement noise standard deviation.\n",
    "    xp : module\n",
    "        Array module.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    particles : array\n",
    "        Updated particles.\n",
    "    weights : array\n",
    "        Updated weights.\n",
    "    \"\"\"\n",
    "    n_particles = particles.shape[0]\n",
    "    state_dim = particles.shape[1]\n",
    "    \n",
    "    # Prediction: propagate particles through dynamics + noise\n",
    "    particles_pred = f_dynamics(particles, xp)\n",
    "    particles_pred += Q_std * xp.random.randn(n_particles, state_dim)\n",
    "    \n",
    "    # Update: compute likelihood weights\n",
    "    z_pred = h_meas(particles_pred, xp)\n",
    "    innovation = z - z_pred\n",
    "    \n",
    "    # Gaussian likelihood (vectorized)\n",
    "    log_likelihood = -0.5 * xp.sum(innovation**2, axis=1) / R_std**2\n",
    "    \n",
    "    # Update weights\n",
    "    log_weights = xp.log(weights + 1e-300) + log_likelihood\n",
    "    log_weights = log_weights - xp.max(log_weights)  # Numerical stability\n",
    "    weights = xp.exp(log_weights)\n",
    "    weights = weights / xp.sum(weights)  # Normalize\n",
    "    \n",
    "    # Compute effective sample size\n",
    "    ess = 1.0 / xp.sum(weights**2)\n",
    "    \n",
    "    # Resample if ESS is too low\n",
    "    if float(ess) < n_particles / 2:\n",
    "        # Systematic resampling\n",
    "        indices = systematic_resample(weights, xp)\n",
    "        particles_pred = particles_pred[indices]\n",
    "        weights = xp.ones(n_particles) / n_particles\n",
    "    \n",
    "    return particles_pred, weights, float(ess)\n",
    "\n",
    "\n",
    "def systematic_resample(weights, xp=np):\n",
    "    \"\"\"\n",
    "    Systematic resampling for particle filters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    weights : array_like\n",
    "        Normalized weights [n_particles].\n",
    "    xp : module\n",
    "        Array module.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    indices : array\n",
    "        Resampled indices.\n",
    "    \"\"\"\n",
    "    n = len(weights)\n",
    "    \n",
    "    # Generate systematic points\n",
    "    positions = (xp.arange(n) + xp.random.rand()) / n\n",
    "    \n",
    "    # Cumulative sum of weights\n",
    "    cumsum = xp.cumsum(weights)\n",
    "    \n",
    "    # Find indices using searchsorted\n",
    "    indices = xp.searchsorted(cumsum, positions)\n",
    "    indices = xp.clip(indices, 0, n - 1)\n",
    "    \n",
    "    return indices\n",
    "\n",
    "\n",
    "print(\"Particle filter functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define nonlinear dynamics and measurement\n",
    "def f_nonlinear(x, xp=np):\n",
    "    \"\"\"Nonlinear dynamics: x_next = x/2 + 25*x/(1+x^2) + 8*cos(1.2*k)\"\"\"\n",
    "    return x / 2 + 25 * x / (1 + x**2 + 1e-10)\n",
    "\n",
    "def h_nonlinear(x, xp=np):\n",
    "    \"\"\"Nonlinear measurement: z = x^2 / 20\"\"\"\n",
    "    return x**2 / 20\n",
    "\n",
    "# Parameters\n",
    "n_particles_list = [1000, 5000, 10000, 50000]\n",
    "if not GPU_AVAILABLE:\n",
    "    n_particles_list = [1000, 5000, 10000]\n",
    "\n",
    "state_dim_pf = 1\n",
    "Q_std = 3.0\n",
    "R_std = 1.0\n",
    "\n",
    "n_steps = 50\n",
    "n_trials = 5\n",
    "\n",
    "cpu_pf_times = []\n",
    "gpu_pf_times = []\n",
    "\n",
    "print(\"Benchmarking particle filter...\")\n",
    "\n",
    "for n_particles in n_particles_list:\n",
    "    # Generate true trajectory and measurements\n",
    "    np.random.seed(42)\n",
    "    true_state = [0.0]\n",
    "    measurements = []\n",
    "    for k in range(n_steps):\n",
    "        x_new = f_nonlinear(np.array([[true_state[-1]]]))[0, 0] + Q_std * np.random.randn()\n",
    "        true_state.append(x_new)\n",
    "        z = h_nonlinear(np.array([[x_new]]))[0, 0] + R_std * np.random.randn()\n",
    "        measurements.append(z)\n",
    "    \n",
    "    # CPU timing\n",
    "    cpu_times_trial = []\n",
    "    for _ in range(n_trials):\n",
    "        particles = np.random.randn(n_particles, state_dim_pf) * 5\n",
    "        weights = np.ones(n_particles) / n_particles\n",
    "        \n",
    "        start = time.time()\n",
    "        for z in measurements:\n",
    "            z_arr = np.array([[z]])\n",
    "            particles, weights, ess = particle_filter_step(\n",
    "                particles, weights, z_arr, f_nonlinear, h_nonlinear, Q_std, R_std, xp=np\n",
    "            )\n",
    "        cpu_times_trial.append(time.time() - start)\n",
    "    cpu_pf_times.append(np.mean(cpu_times_trial))\n",
    "    \n",
    "    if GPU_AVAILABLE:\n",
    "        # GPU timing\n",
    "        gpu_times_trial = []\n",
    "        for _ in range(n_trials):\n",
    "            particles_gpu = cp.random.randn(n_particles, state_dim_pf) * 5\n",
    "            weights_gpu = cp.ones(n_particles) / n_particles\n",
    "            \n",
    "            # Warm up\n",
    "            z_arr = cp.array([[measurements[0]]])\n",
    "            _, _, _ = particle_filter_step(\n",
    "                particles_gpu, weights_gpu, z_arr, f_nonlinear, h_nonlinear, Q_std, R_std, xp=cp\n",
    "            )\n",
    "            cp.cuda.Stream.null.synchronize()\n",
    "            \n",
    "            particles_gpu = cp.random.randn(n_particles, state_dim_pf) * 5\n",
    "            weights_gpu = cp.ones(n_particles) / n_particles\n",
    "            \n",
    "            start = time.time()\n",
    "            for z in measurements:\n",
    "                z_arr = cp.array([[z]])\n",
    "                particles_gpu, weights_gpu, ess = particle_filter_step(\n",
    "                    particles_gpu, weights_gpu, z_arr, f_nonlinear, h_nonlinear, Q_std, R_std, xp=cp\n",
    "                )\n",
    "            cp.cuda.Stream.null.synchronize()\n",
    "            gpu_times_trial.append(time.time() - start)\n",
    "        gpu_pf_times.append(np.mean(gpu_times_trial))\n",
    "        \n",
    "        speedup = cpu_pf_times[-1] / gpu_pf_times[-1]\n",
    "        print(f\"N={n_particles:5d}: CPU={cpu_pf_times[-1]*1e3:7.1f}ms, \"\n",
    "              f\"GPU={gpu_pf_times[-1]*1e3:7.1f}ms, Speedup={speedup:.1f}x\")\n",
    "    else:\n",
    "        print(f\"N={n_particles:5d}: CPU={cpu_pf_times[-1]*1e3:7.1f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize particle filter performance\n",
    "if GPU_AVAILABLE and len(gpu_pf_times) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    ax = axes[0]\n",
    "    ax.loglog(n_particles_list, np.array(cpu_pf_times)*1e3, 'b-o', label='CPU', linewidth=2)\n",
    "    ax.loglog(n_particles_list, np.array(gpu_pf_times)*1e3, 'r-s', label='GPU', linewidth=2)\n",
    "    ax.set_xlabel('Number of Particles')\n",
    "    ax.set_ylabel('Time (ms)')\n",
    "    ax.set_title(f'Particle Filter ({n_steps} steps)')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    ax = axes[1]\n",
    "    speedups = np.array(cpu_pf_times) / np.array(gpu_pf_times)\n",
    "    ax.semilogx(n_particles_list, speedups, 'g-^', linewidth=2, markersize=10)\n",
    "    ax.axhline(y=1, color='k', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Number of Particles')\n",
    "    ax.set_ylabel('Speedup Factor')\n",
    "    ax.set_title('GPU Speedup for Particle Filter')\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 5. Memory Management Best Practices\n",
    "\n",
    "Efficient GPU memory management is crucial for large-scale tracking applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPU_AVAILABLE:\n",
    "    print(\"GPU Memory Management Best Practices\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Memory pools\n",
    "    print(\"\\n1. Memory Pools\")\n",
    "    mempool = cp.get_default_memory_pool()\n",
    "    pinned_mempool = cp.get_default_pinned_memory_pool()\n",
    "    \n",
    "    print(f\"   GPU memory pool used: {mempool.used_bytes() / 1e6:.2f} MB\")\n",
    "    print(f\"   Pinned memory pool used: {pinned_mempool.n_free_blocks()} free blocks\")\n",
    "    \n",
    "    # 2. Pre-allocation\n",
    "    print(\"\\n2. Pre-allocation for Repeated Operations\")\n",
    "    \n",
    "    n = 1000\n",
    "    \n",
    "    # Bad: Allocate new arrays each iteration\n",
    "    start = time.time()\n",
    "    for _ in range(1000):\n",
    "        temp = cp.zeros((n, n))\n",
    "        result = cp.dot(temp, temp)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    time_alloc = time.time() - start\n",
    "    \n",
    "    # Good: Reuse pre-allocated arrays\n",
    "    temp = cp.zeros((n, n))\n",
    "    result = cp.zeros((n, n))\n",
    "    start = time.time()\n",
    "    for _ in range(1000):\n",
    "        cp.dot(temp, temp, out=result)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    time_reuse = time.time() - start\n",
    "    \n",
    "    print(f\"   With allocation each time: {time_alloc*1e3:.1f} ms\")\n",
    "    print(f\"   With pre-allocated arrays: {time_reuse*1e3:.1f} ms\")\n",
    "    print(f\"   Speedup: {time_alloc/time_reuse:.1f}x\")\n",
    "    \n",
    "    # 3. Async transfers\n",
    "    print(\"\\n3. Asynchronous Data Transfers\")\n",
    "    \n",
    "    data_cpu = np.random.randn(10000, 1000)\n",
    "    \n",
    "    # Synchronous\n",
    "    start = time.time()\n",
    "    data_gpu = cp.asarray(data_cpu)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    time_sync = time.time() - start\n",
    "    \n",
    "    # With pinned memory (faster transfers)\n",
    "    data_pinned = cp.cuda.alloc_pinned_memory(data_cpu.nbytes)\n",
    "    data_view = np.frombuffer(data_pinned, dtype=data_cpu.dtype).reshape(data_cpu.shape)\n",
    "    np.copyto(data_view, data_cpu)\n",
    "    \n",
    "    start = time.time()\n",
    "    data_gpu2 = cp.asarray(data_view)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    time_pinned = time.time() - start\n",
    "    \n",
    "    print(f\"   Standard transfer: {time_sync*1e3:.2f} ms\")\n",
    "    print(f\"   Pinned memory transfer: {time_pinned*1e3:.2f} ms\")\n",
    "    \n",
    "    # Clean up\n",
    "    del data_gpu, data_gpu2, temp, result\n",
    "    mempool.free_all_blocks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 6. Integration with pyTCL\n",
    "\n",
    "Here's how to integrate GPU acceleration with existing pyTCL code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from pyTCL\n",
    "from pytcl.dynamic_estimation.kalman import kf_predict, kf_update\n",
    "from pytcl.dynamic_models import constant_velocity_transition\n",
    "\n",
    "def gpu_accelerated_tracking(measurements, F, H, Q, R, x0, P0, use_gpu=True):\n",
    "    \"\"\"\n",
    "    Run Kalman filter tracking with optional GPU acceleration.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    measurements : array_like\n",
    "        List of measurements [n_timesteps, meas_dim].\n",
    "    F, H, Q, R : array_like\n",
    "        Kalman filter matrices.\n",
    "    x0, P0 : array_like\n",
    "        Initial state and covariance.\n",
    "    use_gpu : bool\n",
    "        Whether to use GPU acceleration.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    estimates : array\n",
    "        State estimates [n_timesteps, state_dim].\n",
    "    covariances : array\n",
    "        Covariance estimates [n_timesteps, state_dim, state_dim].\n",
    "    \"\"\"\n",
    "    if use_gpu and GPU_AVAILABLE:\n",
    "        xp = cp\n",
    "        F = cp.asarray(F)\n",
    "        H = cp.asarray(H)\n",
    "        Q = cp.asarray(Q)\n",
    "        R = cp.asarray(R)\n",
    "        x = cp.asarray(x0)\n",
    "        P = cp.asarray(P0)\n",
    "        measurements = cp.asarray(measurements)\n",
    "    else:\n",
    "        xp = np\n",
    "        x = x0.copy()\n",
    "        P = P0.copy()\n",
    "    \n",
    "    estimates = []\n",
    "    covariances = []\n",
    "    \n",
    "    for z in measurements:\n",
    "        # Predict\n",
    "        x, P = kalman_predict(x, P, F, Q, xp)\n",
    "        \n",
    "        # Update\n",
    "        z = z.reshape(-1, 1)\n",
    "        x, P = kalman_update(x, P, z, H, R, xp)\n",
    "        \n",
    "        estimates.append(x.flatten())\n",
    "        covariances.append(P.copy())\n",
    "    \n",
    "    if use_gpu and GPU_AVAILABLE:\n",
    "        estimates = cp.asnumpy(cp.array(estimates))\n",
    "        covariances = cp.asnumpy(cp.array(covariances))\n",
    "    else:\n",
    "        estimates = np.array([xp.asnumpy(e) if hasattr(xp, 'asnumpy') else e for e in estimates])\n",
    "        covariances = np.array([xp.asnumpy(c) if hasattr(xp, 'asnumpy') else c for c in covariances])\n",
    "    \n",
    "    return np.array(estimates), np.array(covariances)\n",
    "\n",
    "print(\"GPU-accelerated tracking function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tracking example\n",
    "# Generate trajectory\n",
    "n_timesteps = 100\n",
    "true_state = []\n",
    "measurements_list = []\n",
    "\n",
    "x_true = np.array([0, 1, 0, 0.5])  # Start at origin, moving NE\n",
    "for t in range(n_timesteps):\n",
    "    true_state.append(x_true.copy())\n",
    "    \n",
    "    # Propagate\n",
    "    x_true = F @ x_true + np.random.multivariate_normal(np.zeros(4), Q * 0.1)\n",
    "    \n",
    "    # Generate noisy measurement\n",
    "    z = H @ x_true + np.random.multivariate_normal(np.zeros(2), R)\n",
    "    measurements_list.append(z)\n",
    "\n",
    "true_state = np.array(true_state)\n",
    "measurements_arr = np.array(measurements_list)\n",
    "\n",
    "# Run tracking\n",
    "x0 = np.array([0, 0, 0, 0]).reshape(-1, 1)\n",
    "P0 = 100 * np.eye(4)\n",
    "\n",
    "estimates_cpu, _ = gpu_accelerated_tracking(\n",
    "    measurements_arr, F, H, Q, R, x0, P0, use_gpu=False\n",
    ")\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    estimates_gpu, _ = gpu_accelerated_tracking(\n",
    "        measurements_arr, F, H, Q, R, x0, P0, use_gpu=True\n",
    "    )\n",
    "\n",
    "print(f\"Tracking complete: {n_timesteps} time steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Trajectory plot\n",
    "ax = axes[0]\n",
    "ax.plot(true_state[:, 0], true_state[:, 2], 'g-', linewidth=2, label='True')\n",
    "ax.scatter(measurements_arr[:, 0], measurements_arr[:, 1], c='gray', s=10, alpha=0.5, label='Measurements')\n",
    "ax.plot(estimates_cpu[:, 0], estimates_cpu[:, 2], 'b--', linewidth=1.5, label='CPU Estimate')\n",
    "if GPU_AVAILABLE:\n",
    "    ax.plot(estimates_gpu[:, 0], estimates_gpu[:, 2], 'r:', linewidth=1.5, label='GPU Estimate')\n",
    "ax.set_xlabel('X Position')\n",
    "ax.set_ylabel('Y Position')\n",
    "ax.set_title('Tracking Results')\n",
    "ax.legend()\n",
    "ax.axis('equal')\n",
    "\n",
    "# Error plot\n",
    "ax = axes[1]\n",
    "error_cpu = np.sqrt((estimates_cpu[:, 0] - true_state[:, 0])**2 + \n",
    "                    (estimates_cpu[:, 2] - true_state[:, 2])**2)\n",
    "ax.plot(error_cpu, 'b-', label='CPU')\n",
    "if GPU_AVAILABLE:\n",
    "    error_gpu = np.sqrt((estimates_gpu[:, 0] - true_state[:, 0])**2 + \n",
    "                        (estimates_gpu[:, 2] - true_state[:, 2])**2)\n",
    "    ax.plot(error_gpu, 'r--', label='GPU')\n",
    "ax.set_xlabel('Time Step')\n",
    "ax.set_ylabel('Position Error')\n",
    "ax.set_title('Tracking Error')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"CPU RMSE: {np.sqrt(np.mean(error_cpu**2)):.4f}\")\n",
    "if GPU_AVAILABLE:\n",
    "    print(f\"GPU RMSE: {np.sqrt(np.mean(error_gpu**2)):.4f}\")\n",
    "    print(f\"Max difference: {np.max(np.abs(estimates_cpu - estimates_gpu)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key takeaways for GPU acceleration:\n",
    "\n",
    "1. **CuPy provides NumPy compatibility** - Easy migration with minimal code changes\n",
    "2. **Batch processing is key** - GPU shines when processing many tracks/particles simultaneously\n",
    "3. **Memory management matters** - Pre-allocate, use pinned memory, free unused memory\n",
    "4. **Transfer overhead** - Minimize CPU↔GPU transfers for best performance\n",
    "5. **Problem size determines speedup** - Small problems may not benefit from GPU\n",
    "\n",
    "### Performance Guidelines\n",
    "\n",
    "| Scenario | Recommendation |\n",
    "|----------|---------------|\n",
    "| < 100 tracks | CPU (transfer overhead dominates) |\n",
    "| 100-1000 tracks | GPU beneficial for complex filters |\n",
    "| > 1000 tracks | GPU strongly recommended |\n",
    "| < 1000 particles | CPU usually faster |\n",
    "| > 10000 particles | GPU provides significant speedup |\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Implement GPU-accelerated UKF with sigma point generation on GPU\n",
    "2. Add GPU memory pooling for repeated filter operations\n",
    "3. Benchmark different matrix sizes to find the CPU/GPU crossover point\n",
    "4. Implement GPU-accelerated JPDA likelihood computation\n",
    "\n",
    "## References\n",
    "\n",
    "1. CuPy Documentation: https://docs.cupy.dev/\n",
    "2. CUDA Programming Guide: https://docs.nvidia.com/cuda/\n",
    "3. Gustafsson, F. (2010). *Particle Methods for Tracking*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
